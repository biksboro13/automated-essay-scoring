{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATASET_DIR = './data/'\n",
    "GLOVE_DIR = './glove.6B/'\n",
    "SAVE_DIR = './'\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "X = pd.read_csv(os.path.join(DATASET_DIR, 'training_set_rel3.tsv'), sep='\\t', encoding='ISO-8859-1')\n",
    "y = X['domain1_score']\n",
    "X = X.dropna(axis=1)\n",
    "X = X.drop(columns=['rater1_domain1', 'rater2_domain1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   domain1_score  \n",
       "0              8  \n",
       "1              9  \n",
       "2              7  \n",
       "3             10  \n",
       "4              8  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimum and Maximum Scores for each essay set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_scores = [-1, 2, 1, 0, 0, 0, 0, 0, 0]\n",
    "maximum_scores = [-1, 12, 6, 3, 3, 4, 4, 30, 60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will preprocess all essays and convert them to feature vectors so that they can be fed into the RNN.\n",
    "\n",
    "These are all helper functions used to clean the essays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def essay_to_wordlist(essay_v, remove_stopwords):\n",
    "    \"\"\"Remove the tagged labels and word tokenize the sentence.\"\"\"\n",
    "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)\n",
    "    words = essay_v.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return (words)\n",
    "\n",
    "def essay_to_sentences(essay_v, remove_stopwords):\n",
    "    \"\"\"Sentence tokenize the essay and call essay_to_wordlist() for word tokenization.\"\"\"\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(essay_v.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(essay_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    return sentences\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    num_words = 0.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            num_words += 1\n",
    "            featureVec = np.add(featureVec,model[word])        \n",
    "    featureVec = np.divide(featureVec,num_words)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(essays, model, num_features):\n",
    "    \"\"\"Main function to generate the word vectors for word2vec model.\"\"\"\n",
    "    counter = 0\n",
    "    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
    "    for essay in essays:\n",
    "        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return essayFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a 2-Layer LSTM Model. \n",
    "\n",
    "Note that instead of using sigmoid activation in the output layer we will use\n",
    "Relu since we are not normalising training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
    "from keras.models import Sequential, load_model, model_from_config\n",
    "import keras.backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Import NumPy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    \n",
    "    #t = Tokenizer()\n",
    "    #t.fit_on_texts(X[\"essay\"])\n",
    "    #word_index = t.word_index\n",
    "    #vocab_size = len(t.word_index) + 1\n",
    "    \n",
    "    #embedding_matrix = np.zeros((vocab_size, num_features))\n",
    "\n",
    "    \"\"\"Define the model.\"\"\"\n",
    "    model = Sequential()\n",
    "    #model.add(Embedding(vocab_size, num_features, weights=[embedding_matrix], trainable=False))\n",
    "    model.add(LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, 300], return_sequences=True))\n",
    "    model.add(LSTM(64, recurrent_dropout=0.4))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model on the dataset.\n",
    "\n",
    "We will use 5-Fold Cross Validation and measure the Quadratic Weighted Kappa for each fold.\n",
    "We will then calculate Average Kappa for all the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------Fold 1--------\n",
      "\n",
      "Training Word2Vec Model...\n",
      "Preprocessing Training Vector...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ovbvs/.local/lib/python2.7/site-packages/ipykernel_launcher.py:34: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "WARNING:tensorflow:From /home/ovbvs/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ovbvs/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/ovbvs/.local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      "10380/10380 [==============================] - 5s 477us/step - loss: 61.3728 - mean_absolute_error: 4.2412\n",
      "Epoch 2/50\n",
      "10380/10380 [==============================] - 3s 282us/step - loss: 38.0010 - mean_absolute_error: 3.4366\n",
      "Epoch 3/50\n",
      "10380/10380 [==============================] - 3s 287us/step - loss: 31.9453 - mean_absolute_error: 3.3360\n",
      "Epoch 4/50\n",
      "10380/10380 [==============================] - 3s 283us/step - loss: 29.4188 - mean_absolute_error: 3.3059\n",
      "Epoch 5/50\n",
      "10380/10380 [==============================] - 3s 287us/step - loss: 28.3249 - mean_absolute_error: 3.2652\n",
      "Epoch 6/50\n",
      "10380/10380 [==============================] - 3s 288us/step - loss: 26.7918 - mean_absolute_error: 3.1715\n",
      "Epoch 7/50\n",
      "10380/10380 [==============================] - 3s 290us/step - loss: 26.8444 - mean_absolute_error: 3.1180\n",
      "Epoch 8/50\n",
      "10380/10380 [==============================] - 3s 288us/step - loss: 24.9919 - mean_absolute_error: 2.9640\n",
      "Epoch 9/50\n",
      "10380/10380 [==============================] - 3s 291us/step - loss: 23.4329 - mean_absolute_error: 2.8179\n",
      "Epoch 10/50\n",
      "10380/10380 [==============================] - 3s 292us/step - loss: 22.0078 - mean_absolute_error: 2.7298\n",
      "Epoch 11/50\n",
      "10380/10380 [==============================] - 3s 290us/step - loss: 19.8496 - mean_absolute_error: 2.5814\n",
      "Epoch 12/50\n",
      "10380/10380 [==============================] - 3s 292us/step - loss: 18.0691 - mean_absolute_error: 2.4610\n",
      "Epoch 13/50\n",
      "10380/10380 [==============================] - 3s 309us/step - loss: 16.1986 - mean_absolute_error: 2.3187\n",
      "Epoch 14/50\n",
      "10380/10380 [==============================] - 3s 296us/step - loss: 15.7028 - mean_absolute_error: 2.2673\n",
      "Epoch 15/50\n",
      "10380/10380 [==============================] - 3s 290us/step - loss: 14.3012 - mean_absolute_error: 2.1701\n",
      "Epoch 16/50\n",
      "10380/10380 [==============================] - 3s 297us/step - loss: 14.2016 - mean_absolute_error: 2.1418\n",
      "Epoch 17/50\n",
      "10380/10380 [==============================] - 3s 298us/step - loss: 13.0542 - mean_absolute_error: 2.0980\n",
      "Epoch 18/50\n",
      "10380/10380 [==============================] - 3s 298us/step - loss: 12.9319 - mean_absolute_error: 2.0643\n",
      "Epoch 19/50\n",
      "10380/10380 [==============================] - 3s 294us/step - loss: 11.9823 - mean_absolute_error: 1.9860\n",
      "Epoch 20/50\n",
      "10380/10380 [==============================] - 3s 296us/step - loss: 11.9017 - mean_absolute_error: 1.9980\n",
      "Epoch 21/50\n",
      "10380/10380 [==============================] - 3s 306us/step - loss: 12.5182 - mean_absolute_error: 1.9876\n",
      "Epoch 22/50\n",
      "10380/10380 [==============================] - 3s 300us/step - loss: 11.1169 - mean_absolute_error: 1.9109\n",
      "Epoch 23/50\n",
      "10380/10380 [==============================] - 3s 297us/step - loss: 11.4008 - mean_absolute_error: 1.9074\n",
      "Epoch 24/50\n",
      "10380/10380 [==============================] - 3s 298us/step - loss: 11.2374 - mean_absolute_error: 1.8769\n",
      "Epoch 25/50\n",
      "10380/10380 [==============================] - 3s 302us/step - loss: 11.5634 - mean_absolute_error: 1.8877\n",
      "Epoch 26/50\n",
      "10380/10380 [==============================] - 3s 303us/step - loss: 10.5705 - mean_absolute_error: 1.8276\n",
      "Epoch 27/50\n",
      "10380/10380 [==============================] - 3s 300us/step - loss: 10.3549 - mean_absolute_error: 1.8171\n",
      "Epoch 28/50\n",
      "10380/10380 [==============================] - 3s 302us/step - loss: 10.6099 - mean_absolute_error: 1.8208\n",
      "Epoch 29/50\n",
      "10380/10380 [==============================] - 3s 306us/step - loss: 10.0117 - mean_absolute_error: 1.7678\n",
      "Epoch 30/50\n",
      "10380/10380 [==============================] - 3s 309us/step - loss: 9.9780 - mean_absolute_error: 1.7815\n",
      "Epoch 31/50\n",
      "10380/10380 [==============================] - 3s 309us/step - loss: 9.2255 - mean_absolute_error: 1.7234\n",
      "Epoch 32/50\n",
      "10380/10380 [==============================] - 3s 309us/step - loss: 9.6014 - mean_absolute_error: 1.7222\n",
      "Epoch 33/50\n",
      "10380/10380 [==============================] - 3s 305us/step - loss: 9.5518 - mean_absolute_error: 1.7209\n",
      "Epoch 34/50\n",
      "10380/10380 [==============================] - 3s 305us/step - loss: 9.4034 - mean_absolute_error: 1.7111\n",
      "Epoch 35/50\n",
      "10380/10380 [==============================] - 3s 309us/step - loss: 9.3680 - mean_absolute_error: 1.7089\n",
      "Epoch 36/50\n",
      "10380/10380 [==============================] - 3s 304us/step - loss: 9.3324 - mean_absolute_error: 1.7051\n",
      "Epoch 37/50\n",
      "10380/10380 [==============================] - 3s 319us/step - loss: 9.3612 - mean_absolute_error: 1.7003\n",
      "Epoch 38/50\n",
      "10380/10380 [==============================] - 3s 333us/step - loss: 8.5945 - mean_absolute_error: 1.6523 3s - loss:  - ETA: 1s - loss: 8.3861 - mean_absol\n",
      "Epoch 39/50\n",
      "10380/10380 [==============================] - 3s 321us/step - loss: 9.1350 - mean_absolute_error: 1.6708 2s - los\n",
      "Epoch 40/50\n",
      "10380/10380 [==============================] - 3s 313us/step - loss: 8.7778 - mean_absolute_error: 1.6457\n",
      "Epoch 41/50\n",
      "10380/10380 [==============================] - 3s 320us/step - loss: 8.3819 - mean_absolute_error: 1.6369\n",
      "Epoch 42/50\n",
      "10380/10380 [==============================] - 3s 319us/step - loss: 8.8425 - mean_absolute_error: 1.6529\n",
      "Epoch 43/50\n",
      "10380/10380 [==============================] - 3s 312us/step - loss: 8.5087 - mean_absolute_error: 1.6303\n",
      "Epoch 44/50\n",
      "10380/10380 [==============================] - 3s 305us/step - loss: 8.6049 - mean_absolute_error: 1.6424\n",
      "Epoch 45/50\n",
      "10380/10380 [==============================] - 3s 305us/step - loss: 8.2616 - mean_absolute_error: 1.6130\n",
      "Epoch 46/50\n",
      "10380/10380 [==============================] - 3s 279us/step - loss: 8.4500 - mean_absolute_error: 1.6219\n",
      "Epoch 47/50\n",
      "10380/10380 [==============================] - 3s 313us/step - loss: 8.4311 - mean_absolute_error: 1.6069\n",
      "Epoch 48/50\n",
      "10380/10380 [==============================] - 3s 279us/step - loss: 8.7064 - mean_absolute_error: 1.6141\n",
      "Epoch 49/50\n",
      "10380/10380 [==============================] - 3s 323us/step - loss: 8.8184 - mean_absolute_error: 1.6235\n",
      "Epoch 50/50\n",
      "10380/10380 [==============================] - 3s 321us/step - loss: 8.3008 - mean_absolute_error: 1.5925\n",
      "Kappa Score: 0.961033873124\n",
      "\n",
      "--------Fold 2--------\n",
      "\n",
      "Training Word2Vec Model...\n",
      "Preprocessing Training Vector...\n",
      "Training...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "10381/10381 [==============================] - 4s 414us/step - loss: 62.2397 - mean_absolute_error: 4.2994\n",
      "Epoch 2/50\n",
      "10381/10381 [==============================] - 3s 275us/step - loss: 38.1012 - mean_absolute_error: 3.4642\n",
      "Epoch 3/50\n",
      "10381/10381 [==============================] - 3s 273us/step - loss: 32.6607 - mean_absolute_error: 3.3823\n",
      "Epoch 4/50\n",
      "10381/10381 [==============================] - 3s 274us/step - loss: 30.5391 - mean_absolute_error: 3.3628\n",
      "Epoch 5/50\n",
      "10381/10381 [==============================] - 3s 273us/step - loss: 29.3083 - mean_absolute_error: 3.3018\n",
      "Epoch 6/50\n",
      "10381/10381 [==============================] - 3s 277us/step - loss: 27.9647 - mean_absolute_error: 3.1672\n",
      "Epoch 7/50\n",
      "10381/10381 [==============================] - 3s 276us/step - loss: 26.5262 - mean_absolute_error: 3.0481\n",
      "Epoch 8/50\n",
      "10381/10381 [==============================] - 3s 281us/step - loss: 23.5191 - mean_absolute_error: 2.8368\n",
      "Epoch 9/50\n",
      "10381/10381 [==============================] - 3s 276us/step - loss: 21.4248 - mean_absolute_error: 2.6964\n",
      "Epoch 10/50\n",
      "10381/10381 [==============================] - 3s 280us/step - loss: 18.6409 - mean_absolute_error: 2.5134\n",
      "Epoch 11/50\n",
      "10381/10381 [==============================] - 3s 278us/step - loss: 17.3170 - mean_absolute_error: 2.4030\n",
      "Epoch 12/50\n",
      "10381/10381 [==============================] - 3s 279us/step - loss: 17.0006 - mean_absolute_error: 2.3589\n",
      "Epoch 13/50\n",
      "10381/10381 [==============================] - 3s 280us/step - loss: 15.2599 - mean_absolute_error: 2.2552\n",
      "Epoch 14/50\n",
      "10381/10381 [==============================] - 3s 279us/step - loss: 15.2226 - mean_absolute_error: 2.2304\n",
      "Epoch 15/50\n",
      "10381/10381 [==============================] - 3s 289us/step - loss: 14.4435 - mean_absolute_error: 2.1716\n",
      "Epoch 16/50\n",
      "10381/10381 [==============================] - 3s 283us/step - loss: 13.8308 - mean_absolute_error: 2.1276\n",
      "Epoch 17/50\n",
      "10381/10381 [==============================] - 3s 295us/step - loss: 13.0276 - mean_absolute_error: 2.0746\n",
      "Epoch 18/50\n",
      "10381/10381 [==============================] - 3s 299us/step - loss: 12.5106 - mean_absolute_error: 2.0459\n",
      "Epoch 19/50\n",
      "10381/10381 [==============================] - 3s 294us/step - loss: 12.1517 - mean_absolute_error: 1.9977\n",
      "Epoch 20/50\n",
      "10381/10381 [==============================] - 3s 301us/step - loss: 12.3960 - mean_absolute_error: 2.0095\n",
      "Epoch 21/50\n",
      "10381/10381 [==============================] - 3s 301us/step - loss: 11.6781 - mean_absolute_error: 1.9376\n",
      "Epoch 22/50\n",
      "10381/10381 [==============================] - 3s 298us/step - loss: 11.3529 - mean_absolute_error: 1.9130\n",
      "Epoch 23/50\n",
      "10381/10381 [==============================] - 3s 304us/step - loss: 10.9793 - mean_absolute_error: 1.8784\n",
      "Epoch 24/50\n",
      "10381/10381 [==============================] - 3s 296us/step - loss: 11.0639 - mean_absolute_error: 1.8621\n",
      "Epoch 25/50\n",
      "10381/10381 [==============================] - 3s 313us/step - loss: 10.8121 - mean_absolute_error: 1.8295\n",
      "Epoch 26/50\n",
      "10381/10381 [==============================] - 3s 306us/step - loss: 10.3788 - mean_absolute_error: 1.8202\n",
      "Epoch 27/50\n",
      "10381/10381 [==============================] - 3s 282us/step - loss: 9.8999 - mean_absolute_error: 1.7596\n",
      "Epoch 28/50\n",
      "10381/10381 [==============================] - 3s 279us/step - loss: 10.5567 - mean_absolute_error: 1.8101\n",
      "Epoch 29/50\n",
      "10381/10381 [==============================] - 3s 291us/step - loss: 10.4826 - mean_absolute_error: 1.8015\n",
      "Epoch 30/50\n",
      "10381/10381 [==============================] - 3s 284us/step - loss: 10.1066 - mean_absolute_error: 1.7551\n",
      "Epoch 31/50\n",
      "10381/10381 [==============================] - 3s 281us/step - loss: 9.4789 - mean_absolute_error: 1.7293\n",
      "Epoch 32/50\n",
      "10381/10381 [==============================] - 3s 291us/step - loss: 9.5926 - mean_absolute_error: 1.7120\n",
      "Epoch 33/50\n",
      "10381/10381 [==============================] - 3s 285us/step - loss: 9.8644 - mean_absolute_error: 1.7532\n",
      "Epoch 34/50\n",
      "10381/10381 [==============================] - 3s 283us/step - loss: 9.3360 - mean_absolute_error: 1.7097\n",
      "Epoch 35/50\n",
      "10381/10381 [==============================] - 3s 278us/step - loss: 9.1450 - mean_absolute_error: 1.7024\n",
      "Epoch 36/50\n",
      "10381/10381 [==============================] - 3s 279us/step - loss: 8.7456 - mean_absolute_error: 1.6729\n",
      "Epoch 37/50\n",
      "10381/10381 [==============================] - 3s 280us/step - loss: 9.2632 - mean_absolute_error: 1.6829\n",
      "Epoch 38/50\n",
      "10381/10381 [==============================] - 3s 276us/step - loss: 8.9102 - mean_absolute_error: 1.6734\n",
      "Epoch 39/50\n",
      "10381/10381 [==============================] - 3s 280us/step - loss: 8.8481 - mean_absolute_error: 1.6524\n",
      "Epoch 40/50\n",
      "10381/10381 [==============================] - 3s 280us/step - loss: 8.7233 - mean_absolute_error: 1.6598\n",
      "Epoch 41/50\n",
      "10381/10381 [==============================] - 3s 278us/step - loss: 8.4984 - mean_absolute_error: 1.6226\n",
      "Epoch 42/50\n",
      "10381/10381 [==============================] - 3s 283us/step - loss: 8.4992 - mean_absolute_error: 1.6129\n",
      "Epoch 43/50\n",
      "10381/10381 [==============================] - 3s 279us/step - loss: 8.7286 - mean_absolute_error: 1.6266\n",
      "Epoch 44/50\n",
      "10381/10381 [==============================] - 3s 278us/step - loss: 8.4324 - mean_absolute_error: 1.6174\n",
      "Epoch 45/50\n",
      "10381/10381 [==============================] - 3s 277us/step - loss: 8.6080 - mean_absolute_error: 1.6265\n",
      "Epoch 46/50\n",
      "10381/10381 [==============================] - 3s 281us/step - loss: 8.0792 - mean_absolute_error: 1.5772\n",
      "Epoch 47/50\n",
      "10381/10381 [==============================] - 3s 279us/step - loss: 8.2209 - mean_absolute_error: 1.5877\n",
      "Epoch 48/50\n",
      "10381/10381 [==============================] - 3s 285us/step - loss: 8.0780 - mean_absolute_error: 1.5842\n",
      "Epoch 49/50\n",
      "10381/10381 [==============================] - 3s 276us/step - loss: 8.3530 - mean_absolute_error: 1.6040\n",
      "Epoch 50/50\n",
      "10381/10381 [==============================] - 3s 272us/step - loss: 7.8437 - mean_absolute_error: 1.5611\n",
      "Kappa Score: 0.958404161196\n",
      "\n",
      "--------Fold 3--------\n",
      "\n",
      "Training Word2Vec Model...\n",
      "Preprocessing Training Vector...\n",
      "Training...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "10381/10381 [==============================] - 5s 438us/step - loss: 60.5650 - mean_absolute_error: 4.2879\n",
      "Epoch 2/50\n",
      "10381/10381 [==============================] - 3s 270us/step - loss: 37.4608 - mean_absolute_error: 3.4527\n",
      "Epoch 3/50\n",
      "10381/10381 [==============================] - 3s 276us/step - loss: 31.7759 - mean_absolute_error: 3.3494\n",
      "Epoch 4/50\n",
      "10381/10381 [==============================] - 3s 291us/step - loss: 29.6298 - mean_absolute_error: 3.3332\n",
      "Epoch 5/50\n",
      "10381/10381 [==============================] - 3s 309us/step - loss: 28.0098 - mean_absolute_error: 3.2752\n",
      "Epoch 6/50\n",
      "10381/10381 [==============================] - 3s 308us/step - loss: 26.6883 - mean_absolute_error: 3.1924\n",
      "Epoch 7/50\n",
      "10381/10381 [==============================] - 3s 308us/step - loss: 25.4843 - mean_absolute_error: 3.0094\n",
      "Epoch 8/50\n",
      "10381/10381 [==============================] - 3s 312us/step - loss: 23.3953 - mean_absolute_error: 2.8529\n",
      "Epoch 9/50\n",
      "10381/10381 [==============================] - 3s 309us/step - loss: 21.3618 - mean_absolute_error: 2.7049\n",
      "Epoch 10/50\n",
      "10381/10381 [==============================] - 3s 315us/step - loss: 18.5479 - mean_absolute_error: 2.5233\n",
      "Epoch 11/50\n",
      "10381/10381 [==============================] - 3s 297us/step - loss: 17.2463 - mean_absolute_error: 2.3900\n",
      "Epoch 12/50\n",
      "10381/10381 [==============================] - 3s 293us/step - loss: 15.5987 - mean_absolute_error: 2.2777\n",
      "Epoch 13/50\n",
      "10381/10381 [==============================] - 3s 293us/step - loss: 14.7917 - mean_absolute_error: 2.2248\n",
      "Epoch 14/50\n",
      "10381/10381 [==============================] - 3s 294us/step - loss: 13.9021 - mean_absolute_error: 2.1519\n",
      "Epoch 15/50\n",
      "10381/10381 [==============================] - 3s 292us/step - loss: 13.0230 - mean_absolute_error: 2.0856\n",
      "Epoch 16/50\n",
      "10381/10381 [==============================] - 3s 305us/step - loss: 12.9086 - mean_absolute_error: 2.0902\n",
      "Epoch 17/50\n",
      "10381/10381 [==============================] - 3s 302us/step - loss: 12.9706 - mean_absolute_error: 2.0578\n",
      "Epoch 18/50\n",
      "10381/10381 [==============================] - 3s 297us/step - loss: 12.5993 - mean_absolute_error: 2.0233\n",
      "Epoch 19/50\n",
      "10381/10381 [==============================] - 3s 297us/step - loss: 11.4724 - mean_absolute_error: 1.9610\n",
      "Epoch 20/50\n",
      "10381/10381 [==============================] - 3s 302us/step - loss: 10.9512 - mean_absolute_error: 1.9216\n",
      "Epoch 21/50\n",
      "10381/10381 [==============================] - 3s 304us/step - loss: 10.8395 - mean_absolute_error: 1.8861\n",
      "Epoch 22/50\n",
      "10381/10381 [==============================] - 3s 304us/step - loss: 10.9215 - mean_absolute_error: 1.8719\n",
      "Epoch 23/50\n",
      "10381/10381 [==============================] - 3s 308us/step - loss: 10.2156 - mean_absolute_error: 1.8234\n",
      "Epoch 24/50\n",
      "10381/10381 [==============================] - 3s 302us/step - loss: 10.3101 - mean_absolute_error: 1.8215\n",
      "Epoch 25/50\n",
      "10381/10381 [==============================] - 3s 303us/step - loss: 10.2659 - mean_absolute_error: 1.8045\n",
      "Epoch 26/50\n",
      "10381/10381 [==============================] - 3s 297us/step - loss: 10.2460 - mean_absolute_error: 1.7893\n",
      "Epoch 27/50\n",
      "10381/10381 [==============================] - 3s 309us/step - loss: 9.8046 - mean_absolute_error: 1.7697\n",
      "Epoch 28/50\n",
      "10381/10381 [==============================] - 3s 305us/step - loss: 9.4791 - mean_absolute_error: 1.7377\n",
      "Epoch 29/50\n",
      "10381/10381 [==============================] - 3s 307us/step - loss: 9.5835 - mean_absolute_error: 1.7290\n",
      "Epoch 30/50\n",
      "10381/10381 [==============================] - 3s 312us/step - loss: 9.5612 - mean_absolute_error: 1.7306\n",
      "Epoch 31/50\n",
      "10381/10381 [==============================] - 3s 300us/step - loss: 9.3004 - mean_absolute_error: 1.7102\n",
      "Epoch 32/50\n",
      "10381/10381 [==============================] - 3s 284us/step - loss: 8.9703 - mean_absolute_error: 1.6865\n",
      "Epoch 33/50\n",
      "10381/10381 [==============================] - 3s 294us/step - loss: 9.2363 - mean_absolute_error: 1.6973\n",
      "Epoch 34/50\n",
      "10381/10381 [==============================] - 3s 279us/step - loss: 8.8106 - mean_absolute_error: 1.6870\n",
      "Epoch 35/50\n",
      "10381/10381 [==============================] - 3s 294us/step - loss: 8.9592 - mean_absolute_error: 1.6729\n",
      "Epoch 36/50\n",
      "10381/10381 [==============================] - 3s 280us/step - loss: 8.9846 - mean_absolute_error: 1.6765\n",
      "Epoch 37/50\n",
      "10381/10381 [==============================] - 3s 286us/step - loss: 8.7153 - mean_absolute_error: 1.6478\n",
      "Epoch 38/50\n",
      "10381/10381 [==============================] - 3s 280us/step - loss: 8.8650 - mean_absolute_error: 1.6520\n",
      "Epoch 39/50\n",
      "10381/10381 [==============================] - 3s 285us/step - loss: 8.4691 - mean_absolute_error: 1.6322\n",
      "Epoch 40/50\n",
      "10381/10381 [==============================] - 3s 279us/step - loss: 8.5575 - mean_absolute_error: 1.6423\n",
      "Epoch 41/50\n",
      "10381/10381 [==============================] - 3s 281us/step - loss: 8.5956 - mean_absolute_error: 1.6244\n",
      "Epoch 42/50\n",
      "10381/10381 [==============================] - 3s 284us/step - loss: 8.7739 - mean_absolute_error: 1.6377\n",
      "Epoch 43/50\n",
      "10381/10381 [==============================] - 3s 282us/step - loss: 8.2158 - mean_absolute_error: 1.6043\n",
      "Epoch 44/50\n",
      "10381/10381 [==============================] - 3s 279us/step - loss: 8.5119 - mean_absolute_error: 1.6160\n",
      "Epoch 45/50\n",
      "10381/10381 [==============================] - 3s 286us/step - loss: 8.1997 - mean_absolute_error: 1.5899\n",
      "Epoch 46/50\n",
      "10381/10381 [==============================] - 3s 282us/step - loss: 8.1182 - mean_absolute_error: 1.5852\n",
      "Epoch 47/50\n",
      "10381/10381 [==============================] - 3s 291us/step - loss: 8.4949 - mean_absolute_error: 1.6070\n",
      "Epoch 48/50\n",
      "10381/10381 [==============================] - 3s 283us/step - loss: 8.5268 - mean_absolute_error: 1.6079\n",
      "Epoch 49/50\n",
      "10381/10381 [==============================] - 3s 282us/step - loss: 7.7241 - mean_absolute_error: 1.5475\n",
      "Epoch 50/50\n",
      "10381/10381 [==============================] - 3s 283us/step - loss: 8.1441 - mean_absolute_error: 1.5644\n",
      "Kappa Score: 0.960886541263\n",
      "\n",
      "--------Fold 4--------\n",
      "\n",
      "Training Word2Vec Model...\n",
      "Preprocessing Training Vector...\n",
      "Training...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "10381/10381 [==============================] - 5s 481us/step - loss: 63.2570 - mean_absolute_error: 4.2672\n",
      "Epoch 2/50\n",
      "10381/10381 [==============================] - 3s 301us/step - loss: 38.9354 - mean_absolute_error: 3.5005\n",
      "Epoch 3/50\n",
      "10381/10381 [==============================] - 3s 307us/step - loss: 32.1205 - mean_absolute_error: 3.3719\n",
      "Epoch 4/50\n",
      "10381/10381 [==============================] - 3s 299us/step - loss: 31.1476 - mean_absolute_error: 3.3585\n",
      "Epoch 5/50\n",
      "10381/10381 [==============================] - 3s 306us/step - loss: 29.4807 - mean_absolute_error: 3.2998\n",
      "Epoch 6/50\n",
      "10381/10381 [==============================] - 3s 301us/step - loss: 28.5454 - mean_absolute_error: 3.2029\n",
      "Epoch 7/50\n",
      "10381/10381 [==============================] - 3s 304us/step - loss: 26.6140 - mean_absolute_error: 3.0302\n",
      "Epoch 8/50\n",
      "10381/10381 [==============================] - 3s 299us/step - loss: 23.4056 - mean_absolute_error: 2.8190\n",
      "Epoch 9/50\n",
      "10381/10381 [==============================] - 3s 310us/step - loss: 20.7211 - mean_absolute_error: 2.6446\n",
      "Epoch 10/50\n",
      "10381/10381 [==============================] - 3s 306us/step - loss: 17.9657 - mean_absolute_error: 2.4701\n",
      "Epoch 11/50\n",
      "10381/10381 [==============================] - 3s 306us/step - loss: 17.0798 - mean_absolute_error: 2.3807\n",
      "Epoch 12/50\n",
      "10381/10381 [==============================] - 3s 309us/step - loss: 15.6593 - mean_absolute_error: 2.2772\n",
      "Epoch 13/50\n",
      "10381/10381 [==============================] - 3s 306us/step - loss: 15.4459 - mean_absolute_error: 2.2534\n",
      "Epoch 14/50\n",
      "10381/10381 [==============================] - 3s 309us/step - loss: 14.5300 - mean_absolute_error: 2.1941\n",
      "Epoch 15/50\n",
      "10381/10381 [==============================] - 3s 312us/step - loss: 14.3140 - mean_absolute_error: 2.1602\n",
      "Epoch 16/50\n",
      "10381/10381 [==============================] - 3s 314us/step - loss: 13.9332 - mean_absolute_error: 2.1246\n",
      "Epoch 17/50\n",
      "10381/10381 [==============================] - 3s 319us/step - loss: 13.5057 - mean_absolute_error: 2.0803\n",
      "Epoch 18/50\n",
      "10381/10381 [==============================] - 3s 317us/step - loss: 12.7160 - mean_absolute_error: 2.0174\n",
      "Epoch 19/50\n",
      "10381/10381 [==============================] - 3s 319us/step - loss: 12.4299 - mean_absolute_error: 1.9968\n",
      "Epoch 20/50\n",
      "10381/10381 [==============================] - 3s 314us/step - loss: 12.2476 - mean_absolute_error: 1.9588\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10381/10381 [==============================] - 3s 297us/step - loss: 12.0274 - mean_absolute_error: 1.9515\n",
      "Epoch 22/50\n",
      "10381/10381 [==============================] - 3s 294us/step - loss: 11.4011 - mean_absolute_error: 1.9019\n",
      "Epoch 23/50\n",
      "10381/10381 [==============================] - 3s 294us/step - loss: 12.1181 - mean_absolute_error: 1.9301\n",
      "Epoch 24/50\n",
      "10381/10381 [==============================] - 3s 295us/step - loss: 11.3347 - mean_absolute_error: 1.8849\n",
      "Epoch 25/50\n",
      "10381/10381 [==============================] - 3s 295us/step - loss: 10.8449 - mean_absolute_error: 1.8400\n",
      "Epoch 26/50\n",
      "10381/10381 [==============================] - 3s 295us/step - loss: 10.4210 - mean_absolute_error: 1.8082\n",
      "Epoch 27/50\n",
      "10381/10381 [==============================] - 3s 294us/step - loss: 10.6094 - mean_absolute_error: 1.8018\n",
      "Epoch 28/50\n",
      "10381/10381 [==============================] - 3s 297us/step - loss: 10.2826 - mean_absolute_error: 1.7820\n",
      "Epoch 29/50\n",
      "10381/10381 [==============================] - 3s 296us/step - loss: 9.8626 - mean_absolute_error: 1.7419\n",
      "Epoch 30/50\n",
      "10381/10381 [==============================] - 3s 295us/step - loss: 9.9088 - mean_absolute_error: 1.7486\n",
      "Epoch 31/50\n",
      "10381/10381 [==============================] - 3s 297us/step - loss: 9.2623 - mean_absolute_error: 1.7195\n",
      "Epoch 32/50\n",
      "10381/10381 [==============================] - 3s 293us/step - loss: 9.7609 - mean_absolute_error: 1.7200\n",
      "Epoch 33/50\n",
      "10381/10381 [==============================] - 3s 293us/step - loss: 9.6603 - mean_absolute_error: 1.7318\n",
      "Epoch 34/50\n",
      "10381/10381 [==============================] - 3s 297us/step - loss: 9.5022 - mean_absolute_error: 1.7153\n",
      "Epoch 35/50\n",
      "10381/10381 [==============================] - 3s 295us/step - loss: 9.8351 - mean_absolute_error: 1.6957\n",
      "Epoch 36/50\n",
      "10381/10381 [==============================] - 3s 295us/step - loss: 9.6266 - mean_absolute_error: 1.7007\n",
      "Epoch 37/50\n",
      "10381/10381 [==============================] - 3s 297us/step - loss: 9.4262 - mean_absolute_error: 1.6854\n",
      "Epoch 38/50\n",
      "10381/10381 [==============================] - 3s 301us/step - loss: 9.0607 - mean_absolute_error: 1.6657\n",
      "Epoch 39/50\n",
      "10381/10381 [==============================] - 3s 297us/step - loss: 9.1536 - mean_absolute_error: 1.6658\n",
      "Epoch 40/50\n",
      "10381/10381 [==============================] - 3s 295us/step - loss: 9.1541 - mean_absolute_error: 1.6627\n",
      "Epoch 41/50\n",
      "10381/10381 [==============================] - 3s 298us/step - loss: 9.0819 - mean_absolute_error: 1.6515\n",
      "Epoch 42/50\n",
      "10381/10381 [==============================] - 3s 295us/step - loss: 8.9893 - mean_absolute_error: 1.6571\n",
      "Epoch 43/50\n",
      "10381/10381 [==============================] - 3s 297us/step - loss: 8.6412 - mean_absolute_error: 1.6392\n",
      "Epoch 44/50\n",
      "10381/10381 [==============================] - 3s 296us/step - loss: 8.6483 - mean_absolute_error: 1.6247\n",
      "Epoch 45/50\n",
      "10381/10381 [==============================] - 3s 295us/step - loss: 8.8315 - mean_absolute_error: 1.6176\n",
      "Epoch 46/50\n",
      "10381/10381 [==============================] - 3s 296us/step - loss: 8.5879 - mean_absolute_error: 1.6114\n",
      "Epoch 47/50\n",
      "10381/10381 [==============================] - 3s 300us/step - loss: 8.7437 - mean_absolute_error: 1.6198\n",
      "Epoch 48/50\n",
      "10381/10381 [==============================] - 3s 297us/step - loss: 8.3895 - mean_absolute_error: 1.5992\n",
      "Epoch 49/50\n",
      "10381/10381 [==============================] - 3s 298us/step - loss: 8.3731 - mean_absolute_error: 1.5901\n",
      "Epoch 50/50\n",
      "10381/10381 [==============================] - 3s 297us/step - loss: 8.3785 - mean_absolute_error: 1.5862\n",
      "Kappa Score: 0.962583269522\n",
      "\n",
      "--------Fold 5--------\n",
      "\n",
      "Training Word2Vec Model...\n",
      "Preprocessing Training Vector...\n",
      "Training...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "10381/10381 [==============================] - 5s 512us/step - loss: 62.5608 - mean_absolute_error: 4.2723\n",
      "Epoch 2/50\n",
      "10381/10381 [==============================] - 3s 303us/step - loss: 39.2104 - mean_absolute_error: 3.4789\n",
      "Epoch 3/50\n",
      "10381/10381 [==============================] - 3s 306us/step - loss: 33.4832 - mean_absolute_error: 3.4023\n",
      "Epoch 4/50\n",
      "10381/10381 [==============================] - 3s 303us/step - loss: 31.3217 - mean_absolute_error: 3.3544\n",
      "Epoch 5/50\n",
      "10381/10381 [==============================] - 3s 304us/step - loss: 28.9423 - mean_absolute_error: 3.2731\n",
      "Epoch 6/50\n",
      "10381/10381 [==============================] - 3s 308us/step - loss: 28.7574 - mean_absolute_error: 3.2437\n",
      "Epoch 7/50\n",
      "10381/10381 [==============================] - 3s 310us/step - loss: 27.3508 - mean_absolute_error: 3.1363\n",
      "Epoch 8/50\n",
      "10381/10381 [==============================] - 3s 307us/step - loss: 25.4718 - mean_absolute_error: 2.9212\n",
      "Epoch 9/50\n",
      "10381/10381 [==============================] - 3s 310us/step - loss: 23.4661 - mean_absolute_error: 2.7759\n",
      "Epoch 10/50\n",
      "10381/10381 [==============================] - 3s 308us/step - loss: 19.5763 - mean_absolute_error: 2.5823\n",
      "Epoch 11/50\n",
      "10381/10381 [==============================] - 3s 312us/step - loss: 18.7629 - mean_absolute_error: 2.4861\n",
      "Epoch 12/50\n",
      "10381/10381 [==============================] - 3s 313us/step - loss: 17.2873 - mean_absolute_error: 2.4052\n",
      "Epoch 13/50\n",
      "10381/10381 [==============================] - 3s 313us/step - loss: 16.5789 - mean_absolute_error: 2.3351\n",
      "Epoch 14/50\n",
      "10381/10381 [==============================] - 3s 315us/step - loss: 15.4839 - mean_absolute_error: 2.2510\n",
      "Epoch 15/50\n",
      "10381/10381 [==============================] - 3s 313us/step - loss: 14.6777 - mean_absolute_error: 2.2024\n",
      "Epoch 16/50\n",
      "10381/10381 [==============================] - 3s 311us/step - loss: 14.5015 - mean_absolute_error: 2.1537\n",
      "Epoch 17/50\n",
      "10381/10381 [==============================] - 3s 314us/step - loss: 13.4222 - mean_absolute_error: 2.0946\n",
      "Epoch 18/50\n",
      "10381/10381 [==============================] - 3s 312us/step - loss: 12.7570 - mean_absolute_error: 2.0681\n",
      "Epoch 19/50\n",
      "10381/10381 [==============================] - 3s 313us/step - loss: 12.8378 - mean_absolute_error: 2.0445\n",
      "Epoch 20/50\n",
      "10381/10381 [==============================] - 3s 312us/step - loss: 12.6484 - mean_absolute_error: 2.0148\n",
      "Epoch 21/50\n",
      "10381/10381 [==============================] - 3s 313us/step - loss: 12.1240 - mean_absolute_error: 1.9745\n",
      "Epoch 22/50\n",
      "10381/10381 [==============================] - 3s 312us/step - loss: 12.2298 - mean_absolute_error: 1.9759\n",
      "Epoch 23/50\n",
      "10381/10381 [==============================] - 3s 315us/step - loss: 12.0433 - mean_absolute_error: 1.9394\n",
      "Epoch 24/50\n",
      "10381/10381 [==============================] - 3s 314us/step - loss: 11.4215 - mean_absolute_error: 1.8987\n",
      "Epoch 25/50\n",
      "10381/10381 [==============================] - 3s 317us/step - loss: 11.4649 - mean_absolute_error: 1.8710\n",
      "Epoch 26/50\n",
      "10381/10381 [==============================] - 3s 316us/step - loss: 10.3954 - mean_absolute_error: 1.8308\n",
      "Epoch 27/50\n",
      "10381/10381 [==============================] - 3s 316us/step - loss: 10.3514 - mean_absolute_error: 1.8273\n",
      "Epoch 28/50\n",
      "10381/10381 [==============================] - 3s 316us/step - loss: 10.5814 - mean_absolute_error: 1.8025\n",
      "Epoch 29/50\n",
      "10381/10381 [==============================] - 3s 313us/step - loss: 10.6311 - mean_absolute_error: 1.8023\n",
      "Epoch 30/50\n",
      "10381/10381 [==============================] - 3s 315us/step - loss: 10.2748 - mean_absolute_error: 1.7814\n",
      "Epoch 31/50\n",
      "10381/10381 [==============================] - 3s 302us/step - loss: 9.6445 - mean_absolute_error: 1.7449\n",
      "Epoch 32/50\n",
      "10381/10381 [==============================] - 3s 300us/step - loss: 9.9099 - mean_absolute_error: 1.7574\n",
      "Epoch 33/50\n",
      "10381/10381 [==============================] - 3s 299us/step - loss: 9.5104 - mean_absolute_error: 1.7142\n",
      "Epoch 34/50\n",
      "10381/10381 [==============================] - 3s 299us/step - loss: 10.2004 - mean_absolute_error: 1.7627\n",
      "Epoch 35/50\n",
      "10381/10381 [==============================] - 3s 308us/step - loss: 9.3852 - mean_absolute_error: 1.7192\n",
      "Epoch 36/50\n",
      "10381/10381 [==============================] - 3s 299us/step - loss: 9.6084 - mean_absolute_error: 1.7186\n",
      "Epoch 37/50\n",
      "10381/10381 [==============================] - 3s 299us/step - loss: 9.4680 - mean_absolute_error: 1.7150\n",
      "Epoch 38/50\n",
      "10381/10381 [==============================] - 3s 298us/step - loss: 9.2953 - mean_absolute_error: 1.6853\n",
      "Epoch 39/50\n",
      "10381/10381 [==============================] - 3s 300us/step - loss: 9.1834 - mean_absolute_error: 1.6869\n",
      "Epoch 40/50\n",
      "10381/10381 [==============================] - 3s 298us/step - loss: 9.1010 - mean_absolute_error: 1.6821\n",
      "Epoch 41/50\n",
      "10381/10381 [==============================] - 3s 301us/step - loss: 8.5413 - mean_absolute_error: 1.6428\n",
      "Epoch 42/50\n",
      "10381/10381 [==============================] - 3s 304us/step - loss: 8.9816 - mean_absolute_error: 1.6526\n",
      "Epoch 43/50\n",
      "10381/10381 [==============================] - 3s 299us/step - loss: 8.7302 - mean_absolute_error: 1.6476\n",
      "Epoch 44/50\n",
      "10381/10381 [==============================] - 3s 304us/step - loss: 8.9416 - mean_absolute_error: 1.6609\n",
      "Epoch 45/50\n",
      "10381/10381 [==============================] - 3s 307us/step - loss: 8.9929 - mean_absolute_error: 1.6497\n",
      "Epoch 46/50\n",
      "10381/10381 [==============================] - 3s 308us/step - loss: 8.7074 - mean_absolute_error: 1.6384\n",
      "Epoch 47/50\n",
      "10381/10381 [==============================] - 3s 301us/step - loss: 8.8058 - mean_absolute_error: 1.6360\n",
      "Epoch 48/50\n",
      "10381/10381 [==============================] - 3s 311us/step - loss: 8.9074 - mean_absolute_error: 1.6381\n",
      "Epoch 49/50\n",
      "10381/10381 [==============================] - 3s 302us/step - loss: 8.7477 - mean_absolute_error: 1.6077\n",
      "Epoch 50/50\n",
      "10381/10381 [==============================] - 3s 300us/step - loss: 8.6045 - mean_absolute_error: 1.6086\n",
      "Kappa Score: 0.963252888111\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True)\n",
    "results = []\n",
    "y_pred_list = []\n",
    "\n",
    "count = 1\n",
    "for traincv, testcv in cv.split(X):\n",
    "    print(\"\\n--------Fold {}--------\\n\".format(count))\n",
    "    X_test, X_train, y_test, y_train = X.iloc[testcv], X.iloc[traincv], y.iloc[testcv], y.iloc[traincv]\n",
    "    \n",
    "    train_essays = X_train['essay']\n",
    "    test_essays = X_test['essay']\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    for essay in train_essays:\n",
    "            # Obtaining all sentences from the training essays.\n",
    "            sentences += essay_to_sentences(essay, remove_stopwords = True)\n",
    "            \n",
    "    # Initializing variables for word2vec model.\n",
    "    num_features = 300 \n",
    "    min_word_count = 40\n",
    "    num_workers = 4\n",
    "    context = 10\n",
    "    downsampling = 1e-3\n",
    "\n",
    "    print(\"Training Word2Vec Model...\")\n",
    "    model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n",
    "\n",
    "    model.init_sims(replace=True)\n",
    "    model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)\n",
    "    \n",
    "    print(\"Preprocessing Training Vector...\")\n",
    "    clean_train_essays = []\n",
    "    \n",
    "    # Generate training and testing data word vectors.\n",
    "    for essay_v in train_essays:\n",
    "        clean_train_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
    "    trainDataVecs = getAvgFeatureVecs(clean_train_essays, model, num_features)\n",
    "    \n",
    "    clean_test_essays = []\n",
    "    for essay_v in test_essays:\n",
    "        clean_test_essays.append(essay_to_wordlist( essay_v, remove_stopwords=True ))\n",
    "    testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n",
    "    \n",
    "    trainDataVecs = np.array(trainDataVecs)\n",
    "    testDataVecs = np.array(testDataVecs)\n",
    "    # Reshaping train and test vectors to 3 dimensions. (1 represnts one timestep)\n",
    "    trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n",
    "    testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    lstm_model = get_model()\n",
    "    lstm_model.fit(trainDataVecs, y_train, batch_size=64, epochs=50)\n",
    "    #lstm_model.load_weights('./model_weights/final_lstm.h5')\n",
    "    y_pred = lstm_model.predict(testDataVecs)\n",
    "    \n",
    "    # Save any one of the 8 models.\n",
    "    if count == 5:\n",
    "         lstm_model.save('./model_weights/final_lstm.h5')\n",
    "    \n",
    "    # Round y_pred to the nearest integer.\n",
    "    y_pred = np.around(y_pred)\n",
    "    \n",
    "    # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n",
    "    result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n",
    "    print(\"Kappa Score: {}\".format(result))\n",
    "    results.append(result)\n",
    "\n",
    "    count += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Average Kappa score after a 5-fold cross validation: ', 0.9612)\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Kappa score after a 5-fold cross validation: \",np.around(np.array(results).mean(),decimals=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
